{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airquality - CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data_preparation/raw_dataset/\", \"AirQualityUCI/AirQualityUCI.csv\")\n",
    "data = pd.read_csv(data_path, delimiter=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0, how=\"all\")\n",
    "data = data.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeslots = {\"morning\"  : \"12:00:00\", \n",
    "             \"afternoon\": \"17:00:00\", \n",
    "             \"evening\"  : \"21:00:00\", \n",
    "             \"night\"    : \"04:00:00\" }\n",
    "\n",
    "def getTimeSlots(value, timeslots):\n",
    "    if value > timeslots['night'] and value < timeslots['morning']:\n",
    "        return 'morning'\n",
    "    elif value > timeslots['morning'] and value < timeslots['afternoon']:\n",
    "        return 'afternoon'\n",
    "    elif value > timeslots['afternoon'] and value < timeslots['evening']:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "import datetime\n",
    "def getDayFromDate(value):\n",
    "    day,month,year = (int(x) for x in value.split('/'))\n",
    "    day_of_week = datetime.date(year, month, day)\n",
    "    return day_of_week.strftime(\"%A\").lower()\n",
    "\n",
    "data['time_slot'] = data['Time'].apply(lambda x: getTimeSlots(x, timeslots))\n",
    "data['day_of_week'] = data['Date'].apply(lambda x: getDayFromDate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Time', 'Date'], axis = 1)\n",
    "data_copy = data.copy()\n",
    "data = data.reindex(columns=['time_slot', 'day_of_week', 'T', 'RH', 'AH',\n",
    "       'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'CO(GT)', 'PT08.S1(CO)'])\n",
    "\n",
    "rename_column = {'CO(GT)': \"true_label\", 'PT08.S1(CO)':'given_label'}\n",
    "data = data.rename(columns=rename_column)\n",
    "data.head(2) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with -200\n",
    "Missing values were marked with -200 in original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.given_label != -200]\n",
    "data = data[data.true_label != -200]\n",
    "data[\"given_label\"].isin([-200]).sum(), data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_given_label = data[\"given_label\"].mean()\n",
    "std_given_label = data[\"given_label\"].std()\n",
    "data['given_label'] = (data[\"given_label\"] - mean_given_label)/std_given_label\n",
    "\n",
    "mean_true_label = data[\"true_label\"].mean()\n",
    "std_true_label = data[\"true_label\"].std()\n",
    "data['true_label'] = (data[\"true_label\"] - mean_true_label)/std_true_label\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airquality - NO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no2 = data_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no2 = data_no2.reindex(columns=['time_slot', 'day_of_week', 'T', 'RH', 'AH',\n",
    "    'PT08.S2(NMHC)', 'PT08.S3(NOx)','PT08.S5(O3)', 'PT08.S1(CO)', 'NO2(GT)', 'PT08.S4(NO2)'])\n",
    "\n",
    "rename_column = { 'NO2(GT)':\"true_label\", 'PT08.S4(NO2)': 'given_label'}\n",
    "data_no2 = data_no2.rename(columns=rename_column)\n",
    "data_no2.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_no2[\"given_label\"].isin([-200]).sum(), data_no2.shape)\n",
    "data_no2 = data_no2[data_no2.given_label != -200]\n",
    "data_no2 = data_no2[data_no2.true_label != -200]\n",
    "data_no2[\"given_label\"].isin([-200]).sum(), data_no2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_given_label = data_no2[\"given_label\"].mean()\n",
    "std_given_label = data_no2[\"given_label\"].std()\n",
    "data_no2['given_label'] = (data_no2[\"given_label\"] - mean_given_label)/std_given_label\n",
    "\n",
    "mean_true_label = data_no2[\"true_label\"].mean()\n",
    "std_true_label = data_no2[\"true_label\"].std()\n",
    "data_no2['true_label'] = (data_no2[\"true_label\"] - mean_true_label)/std_true_label\n",
    "\n",
    "data_no2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Politeness (WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from convokit import Corpus, download\n",
    "wiki_corpus = Corpus(filename=download(\"wikipedia-politeness-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = wiki_corpus.get_utterances_dataframe()\n",
    "\n",
    "def getLabelsList(annot_dict: pd.DataFrame, num_annotators = 5): \n",
    "    alist = []\n",
    "    for item in annot_dict:\n",
    "        len_item = len(list(item.values())) \n",
    "        if  len_item < num_annotators: \n",
    "            to_be_added = num_annotators - len_item\n",
    "            alist.append(list(item.values()))\n",
    "            for _ in range(to_be_added):\n",
    "                alist.append(None)\n",
    "        else:\n",
    "            alist.append(list(item.values())) \n",
    "    return alist \n",
    "\n",
    "list_of_labels = getLabelsList(wiki_data[\"meta.Annotations\"])\n",
    "df_of_labels = pd.DataFrame(list_of_labels, columns=[\"anot_\"+str(x+1) for x in range(5)])\n",
    "df_of_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAgreement(list_of_labels: list, num_annotator = 5):\n",
    "    agreementList = []\n",
    "    temp_dict = {}\n",
    "    for label in list_of_labels:\n",
    "        if label in list(temp_dict.keys()):\n",
    "            temp_dict[label] += 1\n",
    "        else:\n",
    "            temp_dict[label] = 1\n",
    "    for key, val in temp_dict.items():\n",
    "        if val >= (num_annotator//2)+1: \n",
    "            return key      \n",
    "    return None\n",
    "    \n",
    "df_of_labels['agreement'] = df_of_labels.apply(lambda x: getAgreement(x),raw=True, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_labels[\"true_label\"] = df_of_labels[['anot_1', 'anot_2', 'anot_3', 'anot_4', 'anot_5']].median(axis=1, skipna=True)\n",
    "label_dataframe = df_of_labels.copy()\n",
    "df_of_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_of_labels[\"residual_\"+str(i+1)] = df_of_labels[\"anot_\"+str(i+1)] - df_of_labels['true_label']\n",
    "\n",
    "df_of_residual = df_of_labels[['residual_1', 'residual_2', 'residual_3', 'residual_4', 'residual_5']]\n",
    "df_residual_index = df_of_residual.apply(lambda x: np.argmax(abs(x)), axis=1, raw=True)\n",
    "\n",
    "for i in range(df_of_labels.shape[0]):\n",
    "    df_of_labels.loc[i, 'given_label'] = df_of_labels.iloc[i][int(df_residual_index.iloc[i])] \n",
    "\n",
    "new_column = ['anot_1', 'anot_2', 'anot_3', 'anot_4', 'anot_5', \n",
    "                'residual_1', 'residual_2', 'residual_3', 'residual_4','residual_5', \n",
    "                'agreement','true_label', 'given_label']\n",
    "df_of_labels = df_of_labels.reindex(columns=new_column)\n",
    "df_of_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data.reset_index(inplace= True)\n",
    "wiki_data = pd.concat([wiki_data, df_of_labels], axis=1)\n",
    "wiki_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data.drop(['meta.Normalized Score', 'meta.Annotations','meta.parsed', 'vectors'], axis=1, inplace=True)\n",
    "wiki_data = wiki_data.dropna(subset=['agreement'], axis=0)\n",
    "wiki_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toRemove = ['id','anot_1', 'anot_2', 'anot_3', 'anot_4', 'anot_5',\n",
    "            'residual_1', 'residual_2', 'residual_3', 'residual_4', 'residual_5',\n",
    "            'speaker', 'reply_to', 'timestamp', 'conversation_id']\n",
    "wiki_data.drop(columns=toRemove, axis=1, inplace=True)\n",
    "wiki_data.dropna(axis=1)\n",
    "wiki_data.drop(axis=1, columns=['agreement'], inplace=True)\n",
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = wiki_data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If given label are randomly selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataframe.head()\n",
    "modified_label_dataframe = label_dataframe.copy()\n",
    "modified_label_dataframe['given_label'] = label_dataframe[['anot_1', 'anot_2', 'anot_3', 'anot_4', 'anot_5']].apply(lambda x: np.random.choice(x), raw=True, axis=1)\n",
    "modified_label_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_ref = wiki_corpus.get_utterances_dataframe()\n",
    "wiki_data_ref = wiki_data_ref.reset_index(drop=True)\n",
    "wiki_data_ref.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wiki = pd.concat([wiki_data_ref, modified_label_dataframe], axis=1)\n",
    "random_wiki.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wiki = random_wiki.drop(columns=['timestamp', 'speaker', 'reply_to', 'conversation_id','meta.Normalized Score',\n",
    "                                        'meta.Annotations','meta.parsed', 'vectors', 'anot_1', 'anot_2', 'anot_3', 'anot_4','anot_5'])\n",
    "random_wiki.head()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wiki = random_wiki.dropna(axis=0)\n",
    "random_wiki = random_wiki.reset_index(drop=True)\n",
    "random_wiki = random_wiki.drop(columns=['agreement'])\n",
    "random_wiki.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Politeness (Stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "stack_corpus = Corpus(filename=download(\"stack-exchange-politeness-corpus\"))\n",
    "stack_data = stack_corpus.get_utterances_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels = getLabelsList(stack_data[\"meta.Annotations\"])\n",
    "df_annotator = pd.DataFrame(list_of_labels, columns=[\"anot_\"+str(x+1) for x in range(5)])\n",
    "df_annotator.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(columns=['agreement', 'true_label', 'given_label'])\n",
    "df_labels['true_label'] = df_annotator.median(axis=1, skipna=True)\n",
    "df_labels['agreement'] = df_annotator.apply(lambda x: getAgreement(x),raw=True, axis = 1)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_annotation = pd.concat([df_annotator, df_labels], axis=1)\n",
    "final_annotation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when given_label is considered as furthest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_annotator[\"residual_\"+str(i+1)] = df_annotator[\"anot_\"+str(i+1)] - df_labels['true_label']\n",
    "\n",
    "df_residual = df_annotator[['residual_'+str(i+1) for i in range(5)]]\n",
    "df_residual_index = df_residual.apply(lambda x: np.argmax(abs(x)), axis=1, raw=True)\n",
    "for i in range(df_annotator.shape[0]):\n",
    "    df_labels.loc[i, 'given_label'] = df_annotator.iloc[i][int(df_residual_index.iloc[i])] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_data.reset_index(inplace=True)\n",
    "stack_data = pd.concat([stack_data, df_annotator, df_labels], axis=1)\n",
    "stack_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_data.columns\n",
    "stack_data.drop(['meta.Normalized Score', 'meta.Annotations','meta.parsed', 'vectors', 'id'], axis=1, inplace=True)\n",
    "stack_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toRemove = ['timestamp', 'speaker', 'reply_to', 'conversation_id',\n",
    "            'anot_1', 'anot_2', 'anot_3', 'anot_4', 'anot_5',\n",
    "            'residual_1', 'residual_2', 'residual_3', 'residual_4', 'residual_5']\n",
    "stack_data.drop(columns=toRemove, axis=1, inplace=True)\n",
    "stack_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_data = stack_data.dropna(axis=0)\n",
    "stack_data = stack_data.drop(columns=['agreement'])\n",
    "stack_data = stack_data.reset_index(drop=True)\n",
    "stack_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing using Hugging Face Sentence Encoder\n",
    "Our final dataset with this encoding considers random selection of labels as given label.\n",
    "Lets first prepare the dataset with randomly selected given label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_annotation = final_annotation.copy()\n",
    "random_annotation['given_label'] = random_annotation[[col for col in random_annotation.columns if col.startswith('anot')]].apply(lambda x: np.random.choice(x), raw=True, axis=1)\n",
    "random_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_stack = stack_corpus.get_utterances_dataframe()\n",
    "random_stack = random_stack.reset_index(drop=True)\n",
    "random_stack.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_stack = pd.concat([random_stack,random_annotation], axis=1)\n",
    "random_stack = random_stack.drop(columns=['timestamp', 'speaker', 'reply_to', 'conversation_id',\n",
    "       'meta.Normalized Score', 'meta.Annotations','meta.parsed', 'vectors', 'anot_1', 'anot_2', 'anot_3', 'anot_4',\n",
    "       'anot_5'])\n",
    "random_stack.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_stack = random_stack.dropna(axis=0)\n",
    "random_stack = random_stack.drop(columns=['agreement'])\n",
    "random_stack = random_stack.reset_index(drop=True)\n",
    "random_stack.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Hugging face Sentence Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "model.max_seq_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode(random_stack.text)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = random_stack[[\"meta.Binary\",\"true_label\",\"given_label\"]]\n",
    "\n",
    "# Embedding Dataframe\n",
    "col = [\"col_\"+str(i+1) for i in range(embedding.shape[1])]\n",
    "embedding_df = pd.DataFrame(embedding, columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack_random = pd.concat([to_keep, embedding_df], axis=1)\n",
    "final_stack_random.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaphor Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained fastext model\n",
    "import fasttext\n",
    "model = fasttext.load_model(\"./support_files/trained_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath = \"./data_preparation/raw_dataset/Metaphor_Novelty/metaphor_novelty_test.csv\"\n",
    "metaphor_data = pd.read_csv(loadpath)\n",
    "metaphor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitStrings(x: str, combined = True):\n",
    "    temp = []\n",
    "    for substr in x.split(\"_\"):\n",
    "        if not substr.isdigit() and (substr != \"\"):\n",
    "            temp.append(substr.lower())\n",
    "    \n",
    "    return \" \".join(temp) if combined else temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphor_data['text'] = metaphor_data['ID'].apply(lambda x: splitStrings(x))\n",
    "metaphor_data[['word1', 'word2']] = metaphor_data['text'].str.split(expand=True)\n",
    "metaphor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset - Average for given label \n",
    "metaphor_data = metaphor_data.drop([\"ID\", \"text\"], axis=1)\n",
    "metaphor_data['given_label'] = metaphor_data[[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]].apply(lambda x: np.mean(x), axis = 1, raw = True)\n",
    "metaphor_data = metaphor_data.drop([\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"], axis=1)\n",
    "\n",
    "metaphor_data['vector1'] = metaphor_data[\"word1\"].apply(lambda x: model[x])\n",
    "metaphor_data['vector2'] = metaphor_data[\"word2\"].apply(lambda x: model[x])\n",
    "metaphor_data['diff'] = metaphor_data['vector1'] - metaphor_data['vector2']\n",
    "metaphor_data['abs_diff'] = abs(metaphor_data['diff'])\n",
    "metaphor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1_df = metaphor_data.vector1.apply(pd.Series)\n",
    "vector1_df.columns = [\"vector1_\"+str(i+1) for i in range(vector1_df.shape[1]) ]\n",
    "vector2_df = metaphor_data.vector2.apply(pd.Series)\n",
    "vector2_df.columns = [\"vector2_\"+str(i+1) for i in range(vector2_df.shape[1])]\n",
    "\n",
    "vector = pd.concat([vector1_df, vector2_df], axis=1)\n",
    "metaphor_labels = metaphor_data[[\"given_label\", \"Score\"]]\n",
    "final_metaphor = pd.concat([metaphor_labels, vector], axis=1) \n",
    "final_metaphor.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telomere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath = \"./data_preparation/raw_dataset/qPCR telomere/qPCR_telomere.csv\"\n",
    "telomere_data = pd.read_csv(loadpath, index_col=0)\n",
    "telomere_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telomere_cq_telo = telomere_data[[\"true.dna.scg\", \"true.telo.var\", \"true.telo.cq\", \"measured.cq.telo\"]]\n",
    "telomere_cq_telo = telomere_cq_telo.rename(columns={\"true.telo.cq\": \"true_label\", \"measured.cq.telo\":\"given_label\"})\n",
    "print(telomere_cq_telo.shape)\n",
    "telomere_cq_telo.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanlab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Oct 11 2022, 21:39:54) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ec5147ab5bbc74496efa97232afb589cdbea0c15a514f90cf1ef23ccb9e83f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

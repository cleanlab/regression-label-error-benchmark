# Details about datasets in the benchmark

## Common Details
We have modified some of the publicly available datasets in order to standardize them for `regression_label_error_benchmark`. \
Following details are available in every dataset:
- `given_label`: These labels are usually available in real-world datasets. Specific details are mentioned along with dataset details.
- `true_label` : These labels are mostly not available in real-world datasets. 
- `true_error` : Generated by considering histogram and gaussian kernel density analysis on the `true_diff = true_label - given_label`. \
We decided a threshold for each dataset in order to categorize them as error (`1`) or not an error (`0`). This threshold was choosen with attempt to have a symmetrical distribution for datapoints that were considered to be not and error. 

## 1. Air Quality datasets
The datasets in this benchmark are subset of data provided at UCI repository [link](https://archive.ics.uci.edu/ml/datasets/air+quality)

| category  | value     |
|------     | ------    |
| Tag       | `Real`    |
| Modality  | `Tabluar` |

- **Data Domain:** Air quality sensor measurement 
- **Regression Task:** Given information from other sensors and environmental parameters like temperature and humidity, can we predict sensor measurement value of a particular gas type? In this section, we are considering prediction of sensor measurement value of two gases carbon monooxide ($CO$) and Nitrogen dioxide ($NO_2$)

- **Data Collection in Original Dataset:**
    
    - Records environmental parameters like T, AH, and RH
    - hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device.
    - Ground truth is collected using a co-located certified reference analyzer. Finally, they are hourly averaged.
    - The aim of data collection is to develop an approach for neural calibration of a newly designed low-cost multi-sensor device.

- **true_label:** For both gases, ground truth is collected using a certified reference analyzer. This is mounted along with the multi-sensor device.

- **given_label:** For the two gases ($CO$ and $NO_2$), data collected through Air Quality Chemical Multisensor Device is considered as a given label. These sensors are susceptible to sensor drift which can eventually be affecting sensors' concentration estimation capabilities.


- **Paper**:
    S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005, [[Web Link]](http://dx.doi.org/10.1016/j.snb.2007.09.060).


### **Dataset preparation**
- `given_label` and `true_label` in each dataset is derived from corresponding ground truth and sensor response. Details are as below: 

    | Dataset                | given label  | true_label    | shape     |benchmark set    |
    |:---------------        |:------------:|:----------:   |:---------:|:-----------:    |
    | airquality_co_reduced  | PT08.S1(CO)  |CO(GT)         |(7344, 8)  |`low_rsquared_set` |
    | airquality_co_full     | PT08.S1(CO)  |CO(GT)         |(7344, 12) |`high_rsquared_set`|
    | airquality_no2_reduced | PT08.S4(NO2) |NO2(GT)        |(7393, 9)  |`low_rsquared_set `|
    | airquality_no2_full    | PT08.S4(NO2) |NO2(GT)        |(7393, 12) |`high_rsquared_set`|

**Differences:** 
- `airquality_co_full` and `airquality_co_reduced`
    - Same datapoints.
    - *Some features are dropped* in `airquality_co_reduced` to decrease the value of observed r-squared during cross-validation training in autogluon. This is done to create tougher regression problem. 
    
- `airquality_no2_full` and `airquality_no2_reduced`
    - Same datapoints.
    - *Some features are dropped* in `airquality_no2_reduced` to decrease the value of observed r-squared during cross-validation training in autogluon. This is done to create tougher regression problem. 

**Other Details**
- `given_label` and `true_label` are standarized with mean and standard deviation. 
- Features `time_slot` and `day_of_week` are derived from `Time` and `Date` features in original dataset. 
    ```
    timeslots = {"morning"  : "12:00:00", 
                "afternoon": "17:00:00", 
                "evening"  : "21:00:00", 
                "night"    : "04:00:00" }

    day_of_week = replace date with day 
    ```
- Other features are same as provided in original dataset. 

## 2. Stanford Politeness Dataset (Wikipedia edition)
The dataset is derived from data provided at this [link](https://convokit.cornell.edu/documentation/wiki_politeness.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding, Politeness in online conversation
- **Regression Task:** Given a textual content of a conversion, can we predict the level of politeness?

- **Data Collection in Original Dataset**: 
    - Collected from wikipedia talk pages 
    - The original annotations from Amazon Mechanical Turkers for the given utterance. Ratings are on a 1-25 scale.
    - There are total of 5 annotators. 

- **true_label**: 
    Median of all the 5 annotators are considered as true_label. This is same in both the datasets considered in the benchmark. 

- **given_label**: 
    - `stanford_politeness_wiki_furthest`: labels which are furthest from the median of all the 5 annotations. This is done to create a tough regression problem. 
    - `stanford_politeness_wiki_random`: labels are randomly selected among available 5 annotation.  

    

**Dataset Preparation**

- Multi-annotator information from `meta.Annotations` is used to derive `true_label` and `given_label`. 

| Dataset name in benchmark         | given label           | true_label    | shape     |benchmark set    |
|:---------------                   |:------------:         |:----------:   |:---------:|:-----------:    |
| stanford_politeness_wiki_furthest | furthest from median  | median        |(1311, 5)  |`low_rsquared_set` |
| stanford_politeness_wiki_random   | randomly selected     |median         |(1311, 5)  |`high_rsquared_set`|

- We first checked the agreement between the annotators. Only datapoints that had agreement of 3 or more than 3 annotators are kept in the benchmark datasets.

- Some of the features are removed from the dataset as they were not relevant for this task or were mostly empty. 

**Feature Engineering**
1. `stanford_politeness_wiki_furthest`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 
2. `stanford_politeness_wiki_random`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 


## 3. Stanford Politeness Dataset (Stack edition)
The dataset is derived from data provided at this [link](https://convokit.cornell.edu/documentation/stack_politeness.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding, Politeness in online conversation
- **Regression Task:** Given a textual content of a conversion, can we predict the level of politeness?

- **Data Collection in Original Dataset**: 
    - Collected from Stack Exchange requests. 
    - The original annotations from Amazon Mechanical Turkers for the given utterance. Ratings are on a 1-25 scale.
    - There are total of 5 annotators. 

- **true_label**: 
    Median of all the 5 annotators are considered as true_label. This is same in both the datasets considered in the benchmark. 

- **given_label**: 
    - `stanford_politeness_stack_furthest`: labels which are furthest from the median of all the 5 annotators.
    - `stanford_politeness_stack_HFSE_random`: labels are randomly selected among the 5 annotators. 



**Dataset preparation**
- Multi-annotator information from `meta.Annotations` is used to derive `true_label` and `given_label`. 

| Dataset name in benchmark             | given label           | true_label    | shape     |benchmark set    |
|:---------------                       |:------------:         |:----------:   |:---------:|:-----------:    |
| stanford_politeness_stack_furthest    | furthest from median  | median        |(1636, 5)  |`low_rsquared_set` |
| stanford_politeness_stack_HFSE_random | randomly selected     |median         |(1636, 388)  |`high_rsquared_set`|

- We first checked the agreement between the annotators. Only datapoints that had agreement of 3 or more than 3 annotators are kept in the benchmark datasets.

**Feature Engineering**
1. `stanford_politeness_stack_furthest`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 
2. `stanford_politeness_stack_HFSE_random`: We have used the hugging face sentence encoder to get the encoded vectors for text column. Encoding model can be defined as below:
    ```
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    ```

## 4. Metaphor novelity
The dataset is derived from data provided at this [link](http://hilt.cse.unt.edu/resources.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding 
- **Regression Task:** Given two syntactically related words, can we predict metaphor novelity scores?

- **Data Collection in Original Dataset**: 
    - 5 annotators provides politeness rating on scale 0-3. 
    - Five annotations were collected for each instance using Amazon Mechanical Turk.

- **true_label**: ground truth is collected using expert annotators. These are provided with the dataset. Each instance was annotated by two trained annotators who were native English speakers. In cases in which the annotators only disagreed
by a small amount (e.g., a 1 and a 2), their labels were averaged. In cases of larger disagreement (e.g., a 0 and a 3), the annotations were adjudicated by a third party.

- **given_label**: Average of all the five annotations collected through Amazon Mechanical Turk. 


- **Paper:**
    Natalie Parde and Rodney D. Nielsen. A Corpus of Metaphor Novelty Scores for Syntactically-Related Word Pairs. To appear in the Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan, May 7-12, 2018.


### **Dataset preparation**
| Dataset                       |      shape |  benchmark set|
|:----------------              |-----------:| ------------------: |
|metaphor_novelity_concat_average |(3162, 203) |`high_rsquared_set`|

- Orginal dataset provides two words per example. We have used the testing dataset as expert annotations are provided.
- We have used fastText word embeddings to calculate vectors for both the word available in the dataset. 

- `metaphor_novelity_concat_average`: both the word vectors are concatenated to get final feature. They are named vector_1, vector_2..., and so on. 

In case of `metaphor_novelity_concat_average`: 
- `Score` is considered as `true_label`. These are expert annotations available along with dataset. 
- Average of all the annotations is considered as `given_label`.

## 5. qPCR telomere 
The dataset in this section is subset of dataset generated through `R` script provided by authors at [link](https://zenodo.org/record/2615735#.Y0XwdC-B1pQ). 

|category| value|
|--------|------|
|Tag     |`Simulated`|
|Modality|`Tabular`|

- **Data Domain:** Genetics
- **Regression Task:** Simple regression task where independent variables are taken from normal distribution. Dependent variable= $f(\text{independent variables}) + error $. Error is also taken from a normal distribution. 

- **Data Collection in Original Dataset**: 
    - Synthectic data generated using R script. 
    - Authors have provided the formula based on their domain knowledge. 

- **true_label**: Authors try to simulate this using parameters from their domain knowledge. These parameters defines normal distributions that act as independent variables.  

- **given_label**: it is defined as true_label + error. where error is also from normal distribution. 

- **Paper:**
    Nettle D, Seeker L, Nussey D, Froy H, Bateson M (2019) Consequences of measurement error in qPCR telomere data: A simulation study. PLoS ONE 14(5): e0216118. https://doi.org/10.1371/journal.pone.0216118

### **Dataset preparation**
- Other than `given_label` and `true_label`, all the features names are preserved to avoid any contradiction and confusion. 
- `given_label` and `true_label` corresponds to `measured` and `true` columns in original dataset. details are mentioned in below table: 

| Dataset         | given_label    | true_label  |      shape | benchmark set |
|:----------------|:-------------: |:-----------:|-----------:|     -----------:    |
|telomere_cq_telo |measured.cq.telo|true.telo.cq |(10000, 5)  |`both`         |

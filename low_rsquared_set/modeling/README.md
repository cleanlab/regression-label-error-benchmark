# Details about datasets in the benchmark

## Common Details
We have modified some of the publicly available datasets in order to standardize them for `regression_label_error_benchmark`. \
Following details are available in every dataset:
- `given_label`: These labels are usually available in real-world datasets. Specific details are mentioned along with dataset details.
- `true_label` : These labels are mostly not available in real-world datasets. 
- `true_error` : Generated by considering histogram and gaussian kernel density analysis on the `true_diff = true_label - given_label`. \
We decided a threshold for each dataset in order to categorize them as error (`1`) or not an error (`0`). This threshold was choosen with attempt to have a symmetrical distribution for datapoints that were considered to be not and error. 

## 1. Air Quality datasets
The datasets in this benchmark are subset of data provided at UCI repository [link](https://archive.ics.uci.edu/ml/datasets/air+quality)

| category  | value     |
|------     | ------    |
| Tag       | `Real`    |
| Modality  | `Tabluar` |

- **Data Domain:** Air quality sensor measurement 
- **Regression Task:** Given information from other sensors and environmental parameters like temperature and humidity, can we predict sensor measurement value of a particular gas type? In this section, we are considering two predicting sensor measurement value of two gases carbon monooxide (co) and Nitrogen dioxide ($NO_2$)

- **Data Collection in Original Dataset**: 
    - Data collection using 5 different sensors. Also, uses referense analyzer to collect ground truth information. 
    - Records environmental parameters like T, AH, RH 
    - For each sensor they record hourly averaged data sensor response. 

- **given_label**: Response of the sensor that is to be considered for calibration. 
- **true_label**: Data collected from a reference analyzer that is placed along with the sensor under test. Data is collected at same time.  

- **Paper**:
    S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005, [[Web Link]](http://dx.doi.org/10.1016/j.snb.2007.09.060).


### **Dataset preparation**
- `given_label` and `true_label` in each dataset is derived from corresponding ground truth and sensor response. Details are as below: 

    | Dataset                | given label  | true_label    | shape     |benchmark set    |
    |:---------------        |:------------:|:----------:   |:---------:|:-----------:    |
    | airquality_co_reduced  | PT08.S1(CO)  |CO(GT)         |(7344, 8)  |`low_rsquared_set` |
    | airquality_co_full     | PT08.S1(CO)  |CO(GT)         |(7344, 12) |`high_rsquared_set`|
    | airquality_no2_reduced | PT08.S4(NO2) |NO2(GT)        |(7393, 9)  |`low_rsquared_set `|
    | airquality_no2_full    | PT08.S4(NO2) |NO2(GT)        |(7393, 12) |`high_rsquared_set`|

**Differences:** 
- `airquality_co_full` and `airquality_co_reduced`
    - Same datapoints.
    - *Some features are dropped* in `airquality_co_reduced` to decrease the value of observed r-squared during cross-validation training in autogluon. This is done to create tougher regression problem. 
    
- `airquality_no2_full` and `airquality_no2_reduced`
    - Same datapoints.
    - *Some features are dropped* in `airquality_no2_reduced` to decrease the value of observed r-squared during cross-validation training in autogluon. This is done to create tougher regression problem. 

**Other Details**
- `given_label` and `true_label` are standarized with mean and standard deviation. 
- Features `time_slot` and `day_of_week` are derived from `Time` and `Date` features in original dataset. 
    ```
    timeslots = {"morning"  : "12:00:00", 
                "afternoon": "17:00:00", 
                "evening"  : "21:00:00", 
                "night"    : "04:00:00" }

    day_of_week = replace date with day 
    ```
- Other features are same as provided in original dataset. 

## 2. Stanford Politeness Dataset (Wikipedia edition)
The dataset is derived from data provided at this [link](https://convokit.cornell.edu/documentation/wiki_politeness.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding, Politeness in online conversation
- **Regression Task:** Given a textual content of a conversion, can we predict the level of politeness?

- **Data Collection in Original Dataset**: 
    - Collected from wikipedia talk pages 
    - 5 annotators provides politeness rating on scale 0-25. 

- **given_label**: 
    - `stanford_politeness_wiki_furthest`: labels which are furthest from the median of all the 5 annotators
    - `stanford_politeness_wiki_random`: labels are randomly selected among the 5 annotators 

- **true_label**: 
    Median of all the 5 annotators are considered as true_label. This is same in both the datasets considered in the benchmark. 
    

**Dataset Preparation**

- Multi-annotator information from `meta.Annotations` is used to derive `true_label` and `given_label`. 

| Dataset name in benchmark         | given label           | true_label    | shape     |benchmark set    |
|:---------------                   |:------------:         |:----------:   |:---------:|:-----------:    |
| stanford_politeness_wiki_furthest | furthest from median  | median        |(1311, 5)  |`low_rsquared_set` |
| stanford_politeness_wiki_random   | randomly selected     |median         |(1311, 5)  |`high_rsquared_set`|

- We first checked the agreement between the annotators. Only datapoints that had agreement of 3 or more than 3 annotators are kept in the benchmark datasets.

- Some of the features are removed from the dataset as they were not relevant for this task or were mostly empty. 

**Feature Engineering**
1. `stanford_politeness_wiki_furthest`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 
2. `stanford_politeness_wiki_random`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 


## 3. Stanford Politeness Dataset (Stack edition)
The dataset is derived from data provided at this [link](https://convokit.cornell.edu/documentation/stack_politeness.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding, Politeness in online conversation
- **Regression Task:** Given a textual content of a conversion, can we predict the level of politeness?

- **Data Collection in Original Dataset**: 
    - Collected from Stack Exchange requests. 
    - 5 annotators provides politeness rating on scale 0-25. 

- **given_label**: 
    - `stanford_politeness_stack_furthest`: labels which are furthest from the median of all the 5 annotators
    - `stanford_politeness_stack_HFSE_random`: labels are randomly selected among the 5 annotators 

- **true_label**: 
    Median of all the 5 annotators are considered as true_label. This is same in both the datasets considered in the benchmark. 


**Dataset preparation**
- Multi-annotator information from `meta.Annotations` is used to derive `true_label` and `given_label`. 

| Dataset name in benchmark             | given label           | true_label    | shape     |benchmark set    |
|:---------------                       |:------------:         |:----------:   |:---------:|:-----------:    |
| stanford_politeness_stack_furthest    | furthest from median  | median        |(1636, 5)  |`low_rsquared_set` |
| stanford_politeness_stack_HFSE_random | randomly selected     |median         |(1636, 388)  |`high_rsquared_set`|

- We first checked the agreement between the annotators. Only datapoints that had agreement of 3 or more than 3 annotators are kept in the benchmark datasets.

**Feature Engineering**
1. `stanford_politeness_stack_furthest`: No feature engineering has been performed. However, We have dropped columns that had mostly empty. 
2. `stanford_politeness_stack_HFSE_random`: We have used the hugging face sentence encoder to get the encoded vectors for text column. Encoding model can be defined as below:
    ```
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    ```

## 5. Metaphor novelity
The dataset is derived from data provided at this [link](http://hilt.cse.unt.edu/resources.html)

|category| value|
|--------|------|
|Tag     |`Real`|
|Modality|`Text`|

- **Data Domain:** Language understanding 
- **Regression Task:** Given two syntactically related words, can we predict metaphor novelity scores?

- **Data Collection in Original Dataset**: 
    - 5 annotators provides politeness rating on scale 0-3. 

- **given_label**: Average of all the five annotators 

- **true_label**: ground truth is collected using expert annotators. These are provided with the dataset. 

- **Paper:**
    Natalie Parde and Rodney D. Nielsen. A Corpus of Metaphor Novelty Scores for Syntactically-Related Word Pairs. To appear in the Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan, May 7-12, 2018.


### **Dataset preparation**
| Dataset                       |      shape |  benchmark set|
|:----------------              |-----------:| ------------------: |
|metaphor_novelity_concat_average |(3162, 203) |`high_rsquared_set`|

- Orginal dataset provides two words per example. We have used the testing dataset as expert annotations are provided.
- We have used fastText word embeddings to calculate vectors for both the word available in the dataset. 

- `metaphor_novelity_concat_average`: both the word vectors are concatenated to get final feature. They are named vector_1, vector_2..., and so on. 

In case of `metaphor_novelity_concat_average`: 
- `Score` is considered as `true_label`. These are expert annotations available along with dataset. 
- Average of all the annotations is considered as `given_label`.

## 6. qPCR telomere 
The dataset in this section is subset of dataset generated through `R` script provided by authors at [link](https://zenodo.org/record/2615735#.Y0XwdC-B1pQ). 

|category| value|
|--------|------|
|Tag     |`Simulated`|
|Modality|`Tabular`|

- **Data Domain:** Genetics
- **Regression Task:** Simple regression task where independent variables are taken from normal distribution. Dependent variable= $f(\text{independent variables}) + error $. Error is also taken from a normal distribution. 

- **Data Collection in Original Dataset**: Synthectic data generated using R script. Factors domain knowledge to generate this. 

- **given_label**: Error induced out of a normal distribution. it is defined as true_label + error 

- **true_label**: Calculated directly from the mathematical formula where independent variables are taken from a normal distribution. 

- **Paper:**
    Nettle D, Seeker L, Nussey D, Froy H, Bateson M (2019) Consequences of measurement error in qPCR telomere data: A simulation study. PLoS ONE 14(5): e0216118. https://doi.org/10.1371/journal.pone.0216118

### **Dataset preparation**
- Other than `given_label` and `true_label`, all the features names are preserved to avoid any contradiction and confusion. 
- `given_label` and `true_label` corresponds to `measured` and `true` columns in original dataset. details are mentioned in below table: 

| Dataset         | given_label    | true_label  |      shape | benchmark set |
|:----------------|:-------------: |:-----------:|-----------:|     -----------:    |
|telomere_cq_telo |measured.cq.telo|true.telo.cq |(10000, 5)  |`both`         |
